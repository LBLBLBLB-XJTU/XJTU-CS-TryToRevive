# 机器学习笔记（吴恩达，机器学习）

## 机器学习基础

### 2机器学习

#### 2.1监督学习

监督学习是学习一种从X到Y的，或从输入到输出的映射算法。 其中的关键是由编码者提供学习算法示例以供学习。这个学习算法的示例，是指给算法一个正确答案，即给定输入X的正确的标签Y。算法通过学习输入X和正确的标签Y，这样一个数据对，达到只需要输入而不需要输出，并对输出进行合理地预测。

其中一种特殊的监督学习为回归,即试图从无数的可能的数字中预测一个数字.

另一种监督学习为分类,则在可能的几个值中进行预测,即预测类别

#### 2.2无监督学习

无监督学习是在无标签的数据中找到其中的信息

聚类是无监督学习的一种,将未标记的数据放入不同的集群中

异常检测是另一种无监督学习,用于检测异常的数据

降维也是一种无监督学习,它可以将大数据集在尽可能损失少的信息的情况下,压缩成一个小的数据集

### 3线性回归模型

#### 3.1线性回归模型

线性回归模型是一种监督学习,数据将被拟合为一条直线,输入一个数据,一个数字将被预测输出.

用于训练模型的数据集被称为训练集.机器学习中,表示输入的标准符号是小写x,称之为输入变量,或者是特征或输入特征,尝试预测的输出变量(或者称为目标变量)的标准符号为小写的y.一对(x,y)表示是一个训练示例,训练示例的总数则以小写m表示,在训练集中,($x^{(i)}$,$y^{(i)}$)表示第i个训练示例

将训练集提供给算法后,算法将提供一个功能,称为函数f.f将采用新的输入x进行估计或预测,输出出一个预测值$\hat{y}$,这时,函数f被称为模型,x是模型的输入,而预测$\hat{y}$则是模型的输出.注意,y是实际的目标值,而$\hat{y}$则是预测值,可能是真实的值也可能不是真实的值

对于f,考虑到线性回归模型,可以表示为$f_{w,b}(x)=wx+b$,或者简单的是$f(x)=wx+b$.至此,这就是单变量线性回归模型

#### 3.2代价函数

在上面的式子中,为w,b被称为模型的参数,b是截距,w是斜率.对于线性回归,即是找到w和b的过程.对于每个预测值和真实值,计算$(\hat y ^{(i)} - y^{(i)})^2$,并计算所有的预测值和对应的真实值然后求和,即:$\sum_{i=1}^m (\hat y^{(i)} - y^{(i)})^2$,m为训练用例的数量.这个式子会随着m的增大而增大,因此为了避免这一增大,我们采用平均平方误差,即$\frac{1}{m}\sum_{i=1}^m (\hat y^{(i)} - y^{(i)})^2$.此外,我们还会多除上一个2,以维持后续计算的整洁性:$\frac{1}{2m}\sum_{i=1}^m (\hat y^{(i)} - y^{(i)})^2$.即代价函数:$J_{(w,b)} = \frac{1}{2m}\sum_{i=1}^m (\hat y^{(i)} - y^{(i)})^2 $.

对于不同模型,可以采取不同的代价函数,而平均平方误差是线性回归中最常用的代价函数.

我们会希望代价尽量的小,即:$minimizeJ(w,b)$.

### 4梯度下降

#### 4.1梯度下降

梯度下降不仅仅可以用于最小化线性回归的代价函数,还适用于几乎所有的更一般函数$min_{w_1,\ldots,w_n,b}J(w_1,\ldots,w_n,b)$.

算法从随机的$w_i$和$b$的值开始,但是我们常常设置为0.我们不断的改变$w_i$和$b$的值,最终维持在最小值.

但是往往函数不会只有一个最小值,梯度下降能够解决这个问题.梯度下降能够为我们找到多个局部最小值.(好吧,这没有解决问题).梯下下降的思想是:每次向着下降最快的方向前进.如同一个人下山时,会360度张望,找到最陡的方向,然后前进,由此到达山谷.这时我们可以想到,我们会因为处于不同的山头而前往了不同的山谷,即可能有多个极小值.

#### 4.2梯度下降的实现

$w = w - \alpha \frac{\partial}{\partial w}J(w,b)$

* 式子中的$\alpha$称为学习率,通常是0到1之间的一个小整数,如0.01等.$\alpha$代表下降的步幅,如果大,意味着使用巨大的步幅下坡,小则说明以baby的小步幅下降.
* $\frac{d}{dw}J(w,b)$是式子的导数项.可以理解为下降的方向

$b = b - \alpha \frac{\partial}{\partial b}J(w,b)$

* $b$中字符的含义与$w$中的类似

我们将持续这个两个式子,直到$w,b$收敛.为了同时更新$w,b$我们需要同步的进行操作:

$temp\_w = w - \alpha \frac{\partial}{\partial w}J(w,b)$

$temp\_b = b - \alpha \frac{\partial}{\partial b}J(w,b)$

$w = temp\_w$

$b = temp\_b$

注意,应按顺序(或者说$temp$同时更新,$w,b$同时更新)更新,以维持同步.在机器学习的实现中,我们不太在乎是导数还是偏导数,所以此处的$d$和$\partial$意思是一样的.

每次朝着最陡的方向前进,在式子中的表现,即为导数项.又因为使用的是导数,所以可以认为是(极小范围内的)最陡的方向

#### 4.3学习率

如果学习率过低,每次的步幅较小,所以需要的次数过多.如果学习率过大,则步幅过大,可能直接略过了最低点,甚至发散.

学习率考虑的是下降的步伐,因此理想情况下学习率的多少不妨碍达到局部最小.

#### 4.4线性回归中的梯度下降

针对上面$w,b$更新的式子,式子中的导数项如下推导:

$\frac{\partial}{\partial w}J(w,b) = \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i = 1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2 = \frac{\partial}{\partial w} \frac{1}{2m} \sum_{i = 1}^m(wx^{(i)} + b - y^{(i)})^2 =  \frac{1}{2m} \sum_{i = 1}^m(wx^{(i)} + b - y^{(i)})2x^{(i)} = \frac{1}{m} \sum_{i = 1}^m(wx^{(i)} + b - y^{(i)})x^{(i)} = \frac{1}{m} \sum_{i = 1}^m(f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$.

$\frac{\partial}{\partial b}J(w,b) = \frac{\partial}{\partial b} \frac{1}{2m} \sum_{i = 1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2 = \frac{\partial}{\partial b} \frac{1}{2m} \sum_{i = 1}^m(wx^{(i)} + b - y^{(i)})^2 =  \frac{1}{2m} \sum_{i = 1}^m(wx^{(i)} + b - y^{(i)})2 = \frac{1}{m} \sum_{i = 1}^m(wx^{(i)} + b - y^{(i)}) = \frac{1}{m} \sum_{i = 1}^m (f_{w,b}(x^{(i)}) - y^{(i)})$.

将这两个式子塞入梯度下降的式子中即可.对于线性回归,只有一个极小值,所以结果是全局最小值.

实际上这个是批量梯度下降,因为我们在梯度下降时考虑了所有的数据.

### 5多类特征

在上面的情况中,我们的输入x都是一维的.进一步的,我们将维数扩展到多维,用$x_j$表示,$j$最大将是$n$,为总共的维数.而上标则表示第几个用例的特征,为一个行向量,如$x^{(2)} = [1,0,1,4]$.那么在这里有$x_j^{(i)}$表示第$i$个用例的第$j$个特征,如$x_2^{(2)} = 0$.

那么模型改为:

$f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + \ldots + w_nx_n + b$

用$\vec w = [w_1, w_2, \ldots ,w_n]$和$\vec x = [x_1, x_2,\ldots,x_n]$来替换式子,有:

$f_{(\vec w,b)}(\vec x) = \vec w \cdot \vec x +b$

这样的模型称为多元线性回归.

#### 5.1向量化

在线性代数中,向量中的内容从1开始记.而在python中,向量中的内容从0开始记.

对于python代码,可以像下面来表示$\vec w,b,\vec x$(使用Numpy库):

```
w = np.array([1.0,2.5,-3.3])
b = 4
x = np.array([10,20,30])
```

那么对应的模型的公式计算的代码为:

```
f = w[0] * x[0] +
    w[1] * x[1] +
    w[2] * x[2] + b
```

更简单的,使用for循环:

```
f = 0
for j in range(0,n)
    f = f + w[j] * x[j]
f = f + b
```

最终的使用向量来简化以及利用NumPy库的并行能力来加速计算:

```
f = np.dot(w,x) + b
```

在前两段代码中,计算线性的执行:每次对一对$w,x$求和.而向量化的计算中,计算机硬件得到所有的$w$和$x$,利用硬件能力并行的在同一时间对每对$w,x$求和,从而加速速度.

同样的,在进行梯度下降时,导数项也可以用一个向量表示,使得梯度下降时对$w,b$的更新也可以加速

#### 5.2多元线性回归的梯度下降

对于模型:

$f_{(\vec w,b)}(\vec x) = \vec w \cdot \vec x +b$,

则代价函数为:

$J(\vec w,b)$,

则对应的梯度下降公式为:

$w_j = w_j - \alpha \frac{\partial}{\partial w_j} J(\vec w, b) = w_j - \alpha \frac{1}{m} \sum_{i = 1}^m (f_{\vec w,b}(\vec x^{(i)}) - y^{(i)})x_j^{(i)} $和

$b = b - \alpha \frac{\partial}{\partial b} J(\vec w, b) = b - \alpha \frac{1}{m}  \sum_{i = 1}^m (f_{\vec w,b}(\vec x^{(i)}) - y^{(i)})$.

另外的,有一种只适用于线性回归、不需要对$w,b$进行迭代的方法:正态方程法.

### 6特征缩放
