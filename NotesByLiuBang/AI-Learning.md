# 学习

- [学习](#--)
  * [机器学习](#----)
  * [监督学习](#----)
  * [最近邻分类](#-----)
  * [感知器](#---)
  * [支持向量机](#-----)
  * [回归](#--)
  * [损失函数](#----)
  * [过度拟合](#----)
  * [正则化](#---)
  * [scikit-learn](#scikit-learn)
  * [强化学习](#----)
  * [马尔可夫决策过程](#--------)
  * [Q-学习](#q---)
  * [无监督学习](#-----)
    + [聚类](#--)
  * [k均值聚类](#k----)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

## 机器学习

机器学习提供给计算机数据,使用这些数据,计算机能够识别模式并自己执行任务

## 监督学习

监督学习是指计算机学习一个函数,函数将基于数据集(若干输入输出对)把输入映射到输出

分类是监督学习的一种,将输入映射到离散的输出

## 最近邻分类

分类的一种方法是为相关变量分配最接近观察值的值.但是,最近的观察值有时不能表示最直接的结论

为了解决这一局限,有K-最近邻算法,变量将被分配k个最近邻的观察值的均值.k-最近邻算法的缺点是必须计算变量对于所有观察值的距离.

## 感知器

另一种解决分类的方法是创建边界.对于二维数据,我们能够画一条线来区分.

感知器的缺点是,不能整齐和不带错误的分类.

对于感知器,我们的预测函数将是:$h_{\mathbf{w}}(\mathbf{X})=\begin{cases} 1 & \text{if } \mathbf{w} \cdot \mathbf{x} \geq 0 \\ 0 & \text{otherwise} \end {cases}$

这个算法的目标是找到最佳的权重向量,当有新数据时,会对向量进行更新,更新的公式是感知器学习规则:$w_i = w_i + \alpha(y-h_w(x))\times x_i$

这两个公式将会使得每次添加数据,分类越准确.理想情况下,上式中括号内的内容将为零,如果低估值,则括号内为1,权重增加;若高估值,则括号内为-1,权重降低.$\alpha$越高,新数据的对权重的影响越大.

这个过程的结果是一个阈值函数,当估计值超过某个阈值,函数有0至1

对于部分无法表示确定性的问题,则被称为硬阈值.当我们使用一个逻辑函数使可以取0到1之间一个实数时,则被称为软阈值,表示对于估计的信心

## 支持向量机

支持向量机也是分类的一种方法.这个方法在决策边界附近使用了一个附加的支持向量来辅助分类.

理想状态下的决策边界都能进行分类,但是其中的分类能力有好有坏.对于尽可能远离它所分离的两个组的边界,称为最大间隔边界.

另一个支持向量机的好处是,可以表示多于二维的边界,以及非线性的边界

## 回归

相较于分类,回归则会将输入映射到连续的输出中

## 损失函数

损失函数将用来量化回归的好坏,回归越坏,loss越高

对于分类问题,可以使用一个0-1损失函数:

* L(actual,predicted)
  * 0 if actual = predicted
  * 1 otherwise

即在预测正确时不加一,错误时加一.对于函数L有多种形式,如$|actual - predicted)|$,或者$(actual-predicted)^2$.其中的第二个会更狠的惩罚,因为平方会加大差距

## 过度拟合

过度拟合是指模型对数据集分的太好,以至于失去了通用性.此时,损失函数成为了双刃剑

## 正则化

正则化是惩罚复杂过程,支持简单过程的过程,一次来解决过度拟合.

在正则化中,我们通过将假设函数 h 的损失和其复杂性的度量相加来估计假设函数 h 的成本。即

$cost(h) = loss(h) + \lambda complexity(h)$

其中的$\lambda$是一个常数,用来表示复杂性的惩罚作用,$\lambda$越大,复杂性的惩罚越高

去测试是否过度拟合时,有种方法叫做holdout交叉验证.这种方法里,将数据分为训练集和测试集,我们用训练集训练,然后用测试集测试预测的好坏.这种方法的缺点是会有不少数据被用于评估而不是训练.

为了解决这个问题,有k交叉验证.我们把数据分为k组,训练模型k次,每次留下一组用于测试.然后生成k个评估结果,取平均值即可

## scikit-learn

scikit-learn是著名的机器学习的库

## 强化学习

强化学习是极其学习的另一种形式,每次行为都会带来反馈.

学习过程由环境向代理提供状态开始,然后代理不断对状态施加行为,基于行为,环境返回新的状态和反馈

## 马尔可夫决策过程

强化学习可以被视为一种马尔可夫决策过程,具有如下组件:

* 状态集S
* 行为集Actions(S)
* 转移模型P(s'|s,a)
* 反馈函数R(s,a,s')

## Q-学习

Q学习是强化学习的一种,函数Q(s,a)的输出是在状态S下采取行为A的估计

过程以估计为零开始(Q=0),当执行行为,收到反馈后,函数会做两件事:基于当前反馈和未来反馈估计Q(s,a),考虑旧估计和新估计来更新Q(s,a).由此可以改进过去的知识而无需从头开始.即:

***Q(s, a) ⟵ Q(s, a) + α(new value estimate - Q(s, a))***

Q的更新值等于之前的Q值加上一些更新值.这个更新值取决于旧值和新值之间的差距,在乘上一个系数,即学习率.当系数为1,旧值被新值覆盖,为系数为0,新值旧值相同.通过控制这个系数,可以控制旧知识被更新的速度

新值的估计可以被表达为反馈r和未来反馈的估计.为了得到这个未来反馈的估计,我们考虑在进行上一个行为后的新状态,然后加上在新状态下进行行为带来的反馈的最高的估计.这样，我们不仅可以通过它收到的奖励来估计在状态 s 下执行动作 a 的效用，还可以通过下一步的预期效用来估计.这个未来反馈的估计可以用系数$\gamma$来进行权重.由此,有如下公式:

$Q(s, a) \leftarrow Q(s, a) + \alpha \left( [r + \gamma \max_{a'} Q(s', a')] - Q(s, a) \right)$

贪婪决策算法会完全折扣未来的反馈,而不是选择当前状态S中具有最高Q的动作

这就给我带来了对于*探索*和*利用*的权衡.对于贪婪决策,作出的行为取决于以前已经带来的效果,但是同时会导致沿着相同路径一直下去,找不到更佳的路径.而探索,另一方面,则会利用之前为探索的路径,由此会找到更有效率的路径.我们会用**ε-greedy**来表示算法,ε越大,越依赖之前探索,越会进行随机的移动,越小,则越倾向为贪婪决策,进行不随机的移动

另一种强化学习的方式则是对于最终结果进行反馈,但是当复杂性上升时,计算量会大大增加.这时我们会使用函数逼近来解决,这个函数允许我们去近似Q(s,a),由此，算法能够识别哪些动作足够相似，因此它们的估计值也应该相似，并在决策中使用这种启发式方法。

## 无监督学习

在无监督学习中,只有数据被输入,没有标签,AI能够在数据中学习模式

### 聚类

聚类时一种无监督学习,数据被输入,然后由AI分入各个组里

## k均值聚类

k均值聚类会把所有的数据分为k组.每个聚类的中心是一个点,每个聚类会得到离它们最近的点们,在每次分配时,点中心又会迭代带属于聚类的所有点的中央.最终平衡时即得到结果
