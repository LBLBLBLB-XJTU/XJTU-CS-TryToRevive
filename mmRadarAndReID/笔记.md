# 1 毫米波雷达FCMW中IF各种公式的推导
FMCW发出的波的公式为：

$s_t = A \cdot cos(2 \pi f(t)t + \phi)$

其中：
- $A$是信号幅度
- $f(t) = f_0 + k \cdot t$为频率调制函数
- $t$是时间
- $\phi$是相位

假定目标的距离为$R$,反射信号的传播时间为$\tau = \frac{2R}{C}$，则接收到的波的公式为：

$s_r = A \cdot cos(2\pi f(t-\tau)(t-\tau)+\phi)$

代入$\tau$，则有：

$s_r = A \cdot cos(2\pi f(t- \frac{2R}{c})(t - \frac{2R}{c}) + \phi)$

中频信号是发出信号和接收信号的混频IF,公式为：

$IF(t) = s_t \cdot s_r = A^2 \cdot cos(2\pi f(t) t + \phi) \cdot cos(2\pi f(t- \frac{2R}{c})(t - \frac{2R}{c}) + \phi)$

即$IF(t) = \frac{A^2}{2}[cos(2 \pi (f(t)-f(t-\frac{2R}{c}))t + 2\phi) + cos(2 \pi (f(t) + f(t-\frac{2R}{c}))t+2\phi)]$

物体的距离可以通过频率差$\Delta f$来计算：

$R = \frac{c \cdot \Delta f}{2k}$

速度则有多普勒效应决定：

$V=\frac{\Delta f \cdot c}{2f_0}$

信号强度与目标的雷达截面和距离有关：

$P = P_0 \cdot (\frac{\lambda}{4\pi R^2})\cdot \sigma$

其中：

- $P_0$为发射功率
- $\lambda$为波长
- $\sigma$为目标的雷达截面

# 2 为什么稀疏分散的点云，映射到图片后会产生大量冗余数据，以及为什么这些冗余数据会快速消耗网络

点云分布往往不均匀，密集处的多个3D点投射到2D图像时会发生重叠，在图像上只会显示一个点，但是这个点有多个重叠点的信息，
这些信息相同但是被多次编码传输而导致冗余（从进行处理的网络的角度讲，一个点有多个相同的信息，这就是冗余）

# 3 FFT

FFT是一种高效计算离散傅里叶变换（DFT）及其逆变换的算法。DFT是将一个离散信号从时域转换到频域的数学工具，
FFT通过减少计算复杂度，使得DFT的计算速度显著提高。 FFT后得到了频谱，低频分量对应目标的距离。从频谱上得到相位差，由相位差得到角度。

解释：IF是收发的混频，混频中的低频成分对应的频率就是频率差。而多个天线对目标有不同的
角度，体现在相位上，通过相位得到三角关系而得到角度。

# 4 全局特征，局部特征，LSTM简介

局部特征是指在数据集中或输入样本中提取的局部信息，这些特征关注的是数据的某个特定区域或片段，通常用于捕捉细节和具体变化。

全局特征是指在数据集中或输入样本中提取的整体信息，这些特征通常能够描述整个数据的总体趋势或模式，常常用于捕获数据的整体特性。

全局特征往往有局部特征通过池化等降维得到。

长短期记忆网络（LSTM）是一种特殊的递归神经网络（RNN），非常适合处理序列数据。选择LSTM来融合全局特征和局部特征的原因包括：

- 记忆能力：LSTM具有内置的记忆单元，能够捕捉长期依赖关系，使其能够有效地整合全局信息。
- 动态性：LSTM能够处理不同时间步长的信息，适合于局部特征的动态变化。
- 适应性：LSTM可以根据输入数据的变化自动调整其权重，便于融合不同层次的特征。

具体的融合过程如下：

1. 特征提取：
   - 使用卷积神经网络（CNN）等模型提取局部特征。（CNN进行卷积操作，在数据上使用卷积核滑动，关注进行卷积的这一个小的区域）
   - 使用全局平均池化或其他方法提取全局特征。（池化减少了特征图的尺寸空间，将局部信息进行了整合）

2. 特征输入：
   - 将提取的全局特征和局部特征结合在一起，形成一个综合特征向量。
   - 该向量作为LSTM的输入。

3. 序列建模：
   - LSTM处理该综合特征向量，捕捉特征之间的时间依赖关系，生成输出。

进行全局特征和局部特征的融合可以提高模型的表达能力，使其更好地理解数据的整体结构和细节，有助于模型在面对各种输入变化时保持稳定性，
通过融合不同层次的信息，模型能够更全面地理解数据，适应更复杂的任务。

# 5 池化

池化（Pooling）是一种下采样操作，用于减少特征图的尺寸，保留重要信息（注意是采样，有和卷积的区别）

平均池化（Average Pooling）：在特定的窗口内计算所有元素的平均值。 

最大池化（Max Pooling）：在特定的窗口内选择最大值。 

# 6 激活函数

1. Sigmoid 激活函数

    $\sigma (x) = \frac{1}{1+e^{-x}}$

    适用于二分类问题的输出层。 早期的神经网络中广泛使用，但在深层网络中不常用。
2. Tanh 激活函数

   $tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ 

    适用于需要输出为负值和正值的场景，例如循环神经网络（RNN）。 在深层网络中也比Sigmoid更常用。
3. ReLu

    $ReLu(x) = max(0,x)$

    在大多数深层网络中常用，尤其是在卷积神经网络（CNN）中。
4. leaky ReLu

    $LeakyReLu(x) = \begin{equation} \begin{cases} x,x > 0  \\ \alpha x, x \leq 0 \end{cases} \end{equation}$

     在对抗训练和深层网络中，常用于替代ReLU
5. softmax

    $softmax(z_i) = \frac{e^{z_i}}{\sum_j e^{z_i}}$

    其中：
    - $z$是输入向量，包含了神经网络的最后一层线性输出。
    - $z_i$是输入向量中的第 ii 个元素。
    - $j$ 是输入向量中所有元素的索引。

    适用于
    - 多分类问题：Softmax 通常用于多分类问题的输出层，比如图像分类、文本分类等。例如，如果模型的任务是识别图像中是猫、狗还是鸟，Softmax 可以将输出转换为分别为每个类别的概率。

    - 神经网络的最后一层：在神经网络的最后一层，Softmax 常与交叉熵损失函数（Cross-Entropy Loss）一起使用，以评估模型预测与真实标签之间的差异。

# 7 优化函数

1. SGD（Stochastic Gradient Descent）

   特点： 每次使用一个样本或小批量样本来更新模型参数。 简单易实现，广泛应用于各种深度学习任务。

   适用情况： 适用于大规模数据集，尤其是当数据无法完全加载到内存中时。 常用于初学者和简单模型的训练。

2. Adam（Adaptive Moment Estimation）

   特点： 结合了 Momentum 和 RMSprop，使用一阶矩（梯度的均值）和二阶矩（梯度的平方的均值）进行自适应学习率调整。 计算效率高，适应性强，通常能够快速收敛。

   适用情况： 在大多数深度学习任务中广泛使用，尤其适用于大规模数据集和高维参数空间。 很适合需要快速收敛的复杂模型。

3. RMSprop

   特点： 结合了 Adagrad 的优点，通过引入衰减因子来解决 Adagrad 学习率持续下降的问题。 计算每个参数的梯度平方的移动平均，以适应每个参数的自适应学习率。

   适用情况： 特别适合处理非平稳目标的学习，如循环神经网络（RNN）。 在优化复杂模型时表现良好。

# 8 混淆矩阵

混淆矩阵（Confusion Matrix）是用于评估分类模型性能的一种工具，尤其在深度学习和神经网络等领域中，常用于可视化模型的预测结果与实际结果之间的关系。它提供了更详细的信息，而不仅仅是准确率。

混淆矩阵的行表示实际类，列表示预测类，每个单元格表示对应类别的预测结果。常用指标有：准确率，精确率
召回率，F1-score,特异度。

# 9 连续小波变换
连续小波变换（Continuous Wavelet Transform, CWT）是一种信号处理技术，用于分析信号的频率成分及其随时间（或空间）变化的特性。与
传统的傅里叶变换不同，CWT 能够提供时间和频率的局部信息，适用于非平稳信号的分析。

小波是一种具有局部化特性（在时间和频率上）的波形。它通过缩放（尺度变换）和移动（平移变换）来生成一系列的波形。
CWT 是通过将信号与小波函数进行卷积来实现的。在 CWT 中，缩放因子控制小波的宽度，较大对应低频成分，较小对应高频成分。
这使得 CWT 能够在不同的尺度上分析信号。

# 10 Relief特征选择算法
Relief 算法是一种用于特征选择的统计方法，特别适用于处理分类问题。它的主要目标是评估特征在分类任务中的重要性，
通过考虑特征与相似样本之间的关系来选择最佳特征。

Relief算法是一种特征权重算法(Feature weighting algorithms)，根据各个特征和类别的相关性赋予特征不同的权重，
权重小于某个阈值的特征将被移除。Relief算法中特征和类别的相关性是基于特征对近距离样本的区分能力。算法从训练集D中随机选择
一个样本R，然后从和R同类的样本中寻找最近邻样本H，称为Near Hit，从和R不同类的样本中寻找最近邻样本M，称为NearMiss，
然后根据以下规则更新每个特征的权重：如果R和Near Hit在某个特征上的距离小于R和Near Miss上的距离，则说明该特征对区分同类和
不同类的最近邻是有益的，则增加该特征的权重；反之，如果R和Near Hit在某个特征的距离大于R和Near Miss上的距离，说明该特征对
区分同类和不同类的最近邻起负面作用，则降低该特征的权重。以上过程重复m次，最后得到各特征的平均权重。特征的权重越大，
表示该特征的分类能力越强，反之，表示该特征分类能力越弱。

进一步的针对对分类问题，提出了ReliefF算法，区别是：在处理多类问题时，每次从训练样本集中随机取出一个样本R，
然后从和R同类的样本集中找出R的k个近邻样本(near Hits)，从每个R的不同类的样本集中均找出k个近邻样本(near Misses)，
然后更新每个特征的权重。

# 11 稀疏近似算法

稀疏近似算法是一类用于处理高维数据的算法，旨在通过寻找数据的稀疏表示来降低计算复杂度和内存需求。
稀疏表示利用了信号或数据集中实际上只需少量非零或显著特征来准确重构或近似原始信号。


稀疏性指的是在某个基（如特征空间、字典等）下，信号或数据可以用少量非零系数来表示。对于一个信号，
如果在某个表示中只有少数几个元素是非零的，则称该表示是稀疏的。即$x\approx D\cdot w$,$D$是一个基或字典矩阵，$w$为稀疏系数向量。

# 12 微多普勒特征

微多普勒特征（Micro-Doppler Features）是指在微多普勒效应下，由移动物体或其部分（如旋转部件、振动部分）引起的频率变化特征
可以识别和分析动态目标的运动特征。

微多普勒效应是多普勒效应的一个细化，通常涉及目标的内部运动或部分的微小运动。

微多普勒特征包含了目标的运动信息，通常表现为频谱中的特定频率成分，这些成分可以用来区分不同类型的目标。这些特征随时间变化，
反映了目标在运动过程中的动态特征。还可以提供关于目标运动的额外信息，例如旋转速度、振动频率等。

# 13 LSTM

LSTM（长短期记忆网络，Long Short-Term Memory）是一种特殊的递归神经网络（RNN）结构，用于处理和预测时间序列数据。L
STM 旨在解决传统 RNN 在长序列中训练时面临的梯度消失和爆炸问题，从而能够捕获长期依赖关系。

长短时记忆网络的思路比较简单。原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么，假如我们再增加一个状态，
即c，让它来保存长期的状态

LSTM 的核心是其单元结构，包含以下几个主要组件：

- 遗忘门（Forget Gate）：决定了前一状态的信息有多少被保留在当前状态中。
- 输入门（Input Gate）：决定了当前输入的信息有多少被存储在单元状态中。
- 单元状态（Cell State）：在整个序列中传递的信息，类似于网络的记忆。
- 输出门（Output Gate）：决定了当前单元状态的哪些部分将作为输出。

这些门结构通过Sigmoid激活函数来控制信息的流动，使得LSTM能够根据需要选择性地记忆或遗忘信息。